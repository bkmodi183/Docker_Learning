Commands:
1. sudo apt-get update -> to update the system
2. sudo apt-get install docker.io -> to install the docker 
3. sudo usermod -aG docker $USER -> to assign the current user to the docker group so that we do not need to run each command with sudo
4. docker swarm init -> To initialize the master node
4. docker swarm join-token worker -> give the token with which we will be joining the worker node to master node
5. docker info -> will give the infromation about docker info and also about whether the current node is master node or not
6. docker swarm node ls -> gives the list of node connected to the master node. This command can only be run on master node. If we try to run on worker node will throw an      error
7. docker swarn node leave -> will leave the swarm
8. docker service create --name <service_name> --replicas=<no_of_node_to_Replicate> --publish <port_no:port_no> <base_image> -> this command is used to create the service
9. docker ps -> used to check the running container
10. docker service ls -> to check the running services
11. docker stack deploy -c <file.yaml> <custom-stack-name> -> This command is used to deploy the stack where -c means compose file.
12.  docker service scale <service nameâ‰¥=<replicas> -> Scale up your swarm with replicas
13. docker service update --image <image> <service name> -> Update the swarm with new image
14. docker service log <service_name> -> Gives the logs of the service


Questions:

1. Why we need docker swarm
2. why we can't use docker in production
3. what is docker swarm
4. how docker swarm is different than kubernetes -> swarm is native to docker engine and k8 is all together a different tool
5. Note: let's say a docker swarm architecture has 3 worker nodes and all three are running one container of todo-app. let's say worker 1 removed from the swarm by the command
   docker swarm leave then in this case all the data or the task that the particular node is running with will automatically gets deleted. That's the beauty of docker swarm
   that it is  secure no data leakes to worker node.
7. in the above scenario if we kill the container from worker 2 so within a sec the docker master node will automatically creates another container. 
8. docker swarm node ls -> only works on master node. if we run the command on worker node it will show and error response.
9. 

Project one 1: Simple todo-app-service from the docker image

1. Create three nodes -> one master and two worker node
2. docker swarm init -> this command will decide which node is going to be master node
3. Once you run the docker swarm init in one of the node and officaily make it master node, it will generate a token with which we can allow other nodes to join the swarm
   as worker. But before we join worker node we need to make sure the security group of master node allow inbound rule at the port 2377 which allows worker node to connect 
   to the swarm network. 
4. In this way swarm network created.
5. docker service create --name todo-app-service --replicas 3 --publish 8001:8001 bkmodi183/todo-app-repo:latest -> This command creates one service with name todo-app-   service
   here the we have name flag to assign name to the service, replicas i.e. to how many worker nodes we need to replicate the task to run, publish to expose the port on which
   the service is going to run. 
   Note: Here when we declare the port while creating the service the same port gets exposed for worker node as well. We do not need to expose the port for worker node 
         individually. That's the beauty of docker swarm. 
6. docker service ls -> runs on master node and gives the details about service which are running
7. docker ps -> will give the deatisl about container.
8. Now the project is ready here the conatiners are running across all three nodes. 
9. you can kill the container on one of the worker node and within seconds a new coantainer is created by the master node. 


Project 2: Deploy stack 

1. Create a directory -> mkdir deployments
2. create a django-cluster.yaml file with the below details
  version: '3.9'
  services:
  web:
    container_name: Todo-app-django
    image: bkmodi183/todo-app-repo:latest
    ports:
      - 8001:8001
  db:
    container_name: MySQL_DB
    image: mysql:5.7
    ports:
      - 3306:3306
    environment:
      MYSQL_ROOT_PASSWORD: 'test@123'
3. docker stack deploy -c django-cluster.yaml django-stack -> here we have to pass two arguments first one is the yaml file that we want to deply and the second is
                                                              the custom stack name. Also, the -c here means compose file i.e. collect and run.
4. Now check whether the service is be deplyoyed as expected or not.
5.  docker service scale django-stack-db=3 -> Here once you verify all the containers are running as expected you can try to scale the db conatiner with the command
6. Now we are going to do run time changes. Here we want to make change on the front page of the todo-app. Let's say we want to change the tittle of the page from todo-app
   to My-Django-App then first we need to make the changes in the index.html file. Now since the app file changes so we need to again build the image and once the image is 
   bult we need to push the image to the docker hub. Since from the docker hub the swarm stack is fetching the  image details. 
7 After we pushed the updated image to the docker hub. Then we can use the below command to update replicate the changes to the live working todo-app.
  docker service update --image bkmodi183/todo-app-repo:latest django-stack_web 
  Once the above command executes the changes will successfully start reflecting over the web page.

Learnig from Project 2: 
* In this project we learn about how to deply service stack.
* How to scale the service
* We have made changes in the tittle of the index page and then built new image and pushed the updated image to the docker hub. Here the key learning is the running swarm
  pulls the updated images from a central repository call docker hub. So what ever changes that we are performing won't reflect until we pushed the updated image to the 
  docker hub.





